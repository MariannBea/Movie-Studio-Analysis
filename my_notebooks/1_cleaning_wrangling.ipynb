{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tasbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tasbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# to store and manipulate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# for quick data analysis\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# for normalizing profits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# for measuring the accuracy of the predictions with bag of words\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to select most predictive key words\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.blob import TextBlob \n",
    "\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, metrics \n",
    "from sklearn import pipeline, manifold, preprocessing, feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "#creates a profile document which analyses each of the variables in the dataframe \n",
    "# this takes awhile uncomment code below to run\n",
    "\n",
    "# movies_profile = ProfileReport(movies_df, title = \"Movie Report\", explorative=True)\n",
    "# movies_profile.to_file(\"movies.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gained From the Profile\n",
    "\n",
    "**Numerical**\n",
    "\n",
    "director_facebook_likes, *duration*,  actor_1_facebook_likes, actor_2_facebook_likes, actor_3_facebook_likes, cast_total_facebook_likes, movie_facebook_likes, \n",
    "\n",
    "num_voted_users, num_critic_for_reviews,  num_user_for_reviews   \n",
    "\n",
    "gross, budget,\n",
    "\n",
    "*imbd_score*, facenumber_in_poster\n",
    "\n",
    "*Italitized items have close to a normal distribution. The rest of the values are skewed.*\n",
    "\n",
    "**Categorical**\n",
    "\n",
    "*director_name, actor_1_name, actor_2_name, actor_3_name*\n",
    "\n",
    "language, country, content_rating\n",
    "\n",
    "*movie_title, movie_imdb_link*,  aspect_ratio, color\n",
    "\n",
    "*Italitized items have high cardinality.*\n",
    "\n",
    "**Imbalanced Variables**: color, language, country\n",
    "\n",
    "**List Variables**: genres, plot keywords\n",
    "\n",
    "**Large Percentage Missing**: director_facebook_likes, gross, budget, title_year\n",
    "\n",
    "**Extreme Values** : Most of the numerical values have outliers/extreme values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting More Accurate Data\n",
    "\n",
    ": When looking through the budget and profit data to explore some of the more extreme values, \n",
    ": I noticed that the gross value seemed to refer to different IMBD values.  Sometimes it referred to the\n",
    "first weekend in the US and Canada and other times it was close, but not an exact match, to the all time gross.\n",
    "This would obviously affect the results of the income.  To solve this problem, I downloaded world-wide revenue \n",
    "values from [The Movie Database](https://developers.themoviedb.org/3). I also downloaded the budget values. With this information, I calculated the profits for each movie. This was done in this [notebook](https://github.com/MariannBea/Movie-Studio-Analysis/blob/1ea166b5340df00c818f158ac52404fd979c0e74/Notebooks/Get%20Budget%20Info.ipynb)\n",
    "\n",
    "This information will be mereged with the movies_df in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.drop(columns = ['gross', 'budget'], inplace = True)\n",
    "\n",
    "# read budget, revenue and profit data in from csv.\n",
    "budget_data = pd.read_csv('movie_profits.csv')\n",
    "budget_data.drop_duplicates(inplace = True)\n",
    "budget_data.columns = ['movie_title', 'budget', 'revenue', 'profit']\n",
    "\n",
    "movies_df = pd.merge(movies_df, budget_data, on='movie_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse the merged dataframe\n",
    "# uncomment the lines below to run the profile report again\n",
    "\n",
    "# movies_profile = ProfileReport(movie_info, title = \"Merged_Movies\", explorative=True)\n",
    "# movies_profile.to_file(\"merged_movies.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 0 values for budget and revenue. This was the value used by the Movie Database to indicate\n",
    "# a value had not yet been entered.\n",
    "movies_df = movies_df[movies_df.budget != 0]\n",
    "movies_df = movies_df[movies_df.revenue != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profit information was adjusted for inflation over time. \n",
    "\n",
    "Inflation information was found at: https://data.worldbank.org/indicator/FP.CPI.TOTL.ZG \n",
    "\n",
    "Some countries were not present in the data. The countries on the left were give the values for the country or region on the right as they were closest economic match. \n",
    "\n",
    "* Argentina: Latin America & Caribbean \n",
    "* Finland: EU Indonesia: East Asia & Pacific (excluding high income) \n",
    "* South Korea: East Asia & Pacific \n",
    "* Taiwan: China \n",
    "* Thailand: East Asia & Pacific (excluding high income) \n",
    "* West Germany - Russia Soviet Union - Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation = pd.read_csv(\"price_index.csv\")\n",
    "\n",
    "# calculate mean inflation rate for each country.  Use this rate to fill in NaN values    \n",
    "inflation['mean'] = inflation.mean(axis = 1, numeric_only = True)\n",
    "inflation = inflation.set_index('country')\n",
    "\n",
    "# fill missing years with 2005, missing countries with USA (most common)\n",
    "movies_df['title_year'].fillna(2005, inplace = True)\n",
    "movies_df['country'].fillna('USA', inplace = True)\n",
    "\n",
    "#determine inflation rate for each movie, movies before 1960 given mean inflation rate\n",
    "movies_df['title_year'] = movies_df['title_year'].astype(int)\n",
    "\n",
    "for index, year, country in movies_df[['title_year', 'country']].itertuples():\n",
    "    stryear = str(year)\n",
    "    strcountry = str(country)\n",
    "    if year >= 1960:\n",
    "        movies_df['rate'] = inflation.loc[[strcountry],[stryear]].iloc[0][0]\n",
    "    else:\n",
    "        movies_df['rate'] = inflation.loc[[strcountry],['mean']].iloc[0][0]\n",
    "\n",
    "# find budget, gross and profits adjusted for inflation\n",
    "movies_df['budget_infl'] = (movies_df['budget']/movies_df['rate']) * 100\n",
    "movies_df['gross_infl'] = (movies_df['revenue']/movies_df['rate']) * 100\n",
    "movies_df['profit_infl'] = (movies_df['profit']/movies_df['rate']) * 100\n",
    "\n",
    "movies_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11929309505.107433\n"
     ]
    }
   ],
   "source": [
    "# find the firstand third quartile amounts to use to categorize the values\n",
    "Q1 = movies_df['profit_infl'].quantile(0.25)\n",
    "Q3 = movies_df['profit_infl'].quantile(0.75) \n",
    "\n",
    "#normalize the inflation-adjusted profit values\n",
    "scaler = MinMaxScaler()\n",
    "movies_df['normalized_profit'] = scaler.fit_transform(movies_df['profit_infl'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Profits    1160\n",
      "Success          598\n",
      "Failure          593\n",
      "Name: profit_str, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# divide the profits into categories to use when creating decision trees\n",
    "movies_df['profit_str'] = None\n",
    "\n",
    "movies_df['profit_str'] = np.where(movies_df['profit_infl'] >= Q3, 'Success', movies_df['profit_str'])\n",
    "\n",
    "movies_df['profit_str'] = np.where(movies_df['profit_infl'].between(Q1, Q3),\n",
    "                                              'Some Profits', movies_df['profit_str'])\n",
    "movies_df['profit_str'] = np.where(movies_df['profit_infl'].between(0, Q1),\n",
    "                                              'Low Profits', movies_df['profit_str'])\n",
    "movies_df['profit_str'] = np.where(movies_df['profit_infl'] <= 0,\n",
    "                                              'Failure', movies_df['profit_str'])\n",
    "\n",
    "#drop any values that were not filled\n",
    "movies_df.dropna(how='any', inplace=True)\n",
    "\n",
    "#check to see how many items are in each category\n",
    "print(movies_df['profit_str'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first, second and third quartile of the inflation-adjusted budget\n",
    "Q1 = movies_df['budget_infl'].quantile(0.25)\n",
    "Q2 = movies_df['budget_infl'].quantile(0.50)\n",
    "Q3 = movies_df['budget_infl'].quantile(0.75)\n",
    "\n",
    "# divide budget into categories to use in decision tree analysis\n",
    "movies_df['budget_quartile'] = None\n",
    "\n",
    "movies_df['budget_quartile'] = np.where(movies_df['budget_infl'] >= Q3, 100, movies_df['budget_quartile'])\n",
    "\n",
    "movies_df['budget_quartile'] = np.where(movies_df['budget_infl'].between(Q2, Q3),\n",
    "                                              75, movies_df['budget_quartile'])\n",
    "movies_df['budget_quartile'] = np.where(movies_df['budget_infl'].between(Q1, Q2),\n",
    "                                              50, movies_df['budget_quartile'])\n",
    "movies_df['budget_quartile'] = np.where(movies_df['budget_infl'].between(0, Q1),\n",
    "                                              25, movies_df['budget_quartile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the 'genres' column into binary columns for each genre\n",
    "\n",
    "# create a new column, transform 'genres' into  a lists, \n",
    "# store the lists in the new columns\n",
    "movies_df['genre_list'] = movies_df['genres'].copy()\n",
    "movies_df['genre_list'] = movies_df['genre_list'].str.split('|')\n",
    "movies_df['genre_count'] =  movies_df['genre_list'].str.len()\n",
    "\n",
    "# create boolean columns for each genre\n",
    "\n",
    "genres = set([item for lists in movies_df['genre_list'] for item in lists])\n",
    "\n",
    "# Below code was taken from: \n",
    "# https://towardsdatascience.com/dealing-with-list-values-in-pandas-dataframes-a177e534f173\n",
    "# Create empty dict\n",
    "genre_dict = {}\n",
    "\n",
    "# Loop through all the tags\n",
    "for i, item in enumerate(genres):\n",
    "    # Apply boolean mask\n",
    "    genre_dict[item] = movies_df['genre_list'].apply(lambda x: item in x)\n",
    "\n",
    "# Return the results as a dataframe, change True and false values to 0 and 1\n",
    "genre_frame =  pd.DataFrame(genre_dict)\n",
    "genre_frame = genre_frame.astype(int)\n",
    "\n",
    "#merge the genre dataframe back with the original one\n",
    "movies_df = pd.merge(movies_df, genre_frame, left_index = True, right_index = True)\n",
    "# delete list so that duplicates can be dropped\n",
    "movies_df.drop(columns = ['genre_list'], inplace = True)\n",
    "movies_df.drop_duplicates(inplace = True)\n",
    "\n",
    "#recreate list\n",
    "movies_df['genre_list'] = movies_df['genres'].copy()\n",
    "movies_df['genre_list'] = movies_df['genre_list'].str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consolidate ratings into smaller categories\n",
    "movies_df['content_rating'].replace({'Not Rated': 'Unrated', 'Approved': 'Unrated', \n",
    "                                           'TV-G': 'G',  'TV-PG': 'PG', 'TV-MA': 'R',\n",
    "                                           'TV-Y': 'G', 'TV-14': 'PG-13', 'Passed': 'Unrated',\n",
    "                                           'TV-Y7': 'PG', 'M': 'PG', 'GP': 'PG'}, inplace = True)\n",
    "movies_df['ratings'] = movies_df['content_rating'].copy()\n",
    "movies_df = pd.get_dummies(movies_df, columns=['ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many movies a director or actor took part in up to the current movie\n",
    "\n",
    "# sort movies by year so that the earliest movie a director made will have a count of one.\n",
    "movies_df = movies_df.sort_values(by = ['title_year']).reset_index(drop = True)\n",
    "\n",
    "# dictionary to store the count for each director\n",
    "director_count = {}\n",
    "\n",
    "# The first time a director is encountered in the dataframe, they will be given a count of one.  \n",
    "# Each additional movie will add to the total\n",
    "for index, title, director, year in movies_df[['movie_title','director_name', 'title_year']].itertuples():\n",
    "    if director in director_count:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'director_count'] = (director_count[director] + 1)\n",
    "        director_count[director] += 1\n",
    "    else:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'director_count'] = 1\n",
    "        director_count[director] = 1\n",
    "        \n",
    "# indicate whether or not a director made 5 or more movies        \n",
    "movies_df['directed_5_plus'] = 'no'\n",
    "movies_df['directed_5_plus'] = np.where(movies_df['director_count'] >= 5, 'yes', movies_df['directed_5_plus'])\n",
    "movies_df = pd.get_dummies(movies_df, columns=['directed_5_plus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code replicates the code above, but for each actor\n",
    "actor_count = {}\n",
    "\n",
    "for index, title, actor1, actor2, actor3 in movies_df[['movie_title','actor_1_name', 'actor_2_name', 'actor_3_name']].itertuples():\n",
    "    if actor1 in actor_count:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'actor1_count'] = (actor_count[actor1] + 1)\n",
    "        actor_count[actor1] += 1\n",
    "    else:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'actor1_count'] = 1\n",
    "        actor_count[actor1] = 1\n",
    "    if actor2 in actor_count:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'actor2_count'] = (actor_count[actor2] + 1)\n",
    "        actor_count[actor1] += 1\n",
    "    else:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'actor2_count'] = 1\n",
    "        actor_count[actor2] = 1\n",
    "    if actor3 in actor_count:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'actor3_count'] = (actor_count[actor3] + 1)\n",
    "        actor_count[actor1] += 1\n",
    "    else:\n",
    "        movies_df.loc[movies_df['movie_title'] == title,'actor3_count'] = 1\n",
    "        actor_count[actor3] = 1\n",
    "        \n",
    "movies_df['actor_count'] = movies_df['actor1_count'] + movies_df['actor2_count'] + movies_df['actor3_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = movies_df[['movie_title','profit_str', 'plot_keywords']]\n",
    "\n",
    "## rename columns\n",
    "dtf = dtf.rename(columns={\"profit_str\":\"y\", \"plot_keywords\":\"text\"})\n",
    "\n",
    "#https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "   \n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "            \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "        \n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "     \n",
    "     ## remove duplicate words\n",
    "    lst_text = set(lst_text)\n",
    "    \n",
    "#     ## back to string from list\n",
    "    text = \", \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords= lst_stopwords))\n",
    "\n",
    "dtf.drop(columns = ['text'], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Failure:\n",
      "  . selected features: 23\n",
      "  . top features: sex,fighter,friendship friend,immigrant,bus\n",
      " \n",
      "# Some Profits:\n",
      "  . selected features: 4\n",
      "  . top features: bear,epic,artist,combat\n",
      " \n",
      "# Success:\n",
      "  . selected features: 51\n",
      "  . top features: epic,tale,bear,fish,british\n",
      " \n",
      "Accuracy: 0.5\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Failure       0.45      0.03      0.05       183\n",
      "Some Profits       0.51      0.94      0.66       356\n",
      "     Success       0.38      0.09      0.14       167\n",
      "\n",
      "    accuracy                           0.50       706\n",
      "   macro avg       0.45      0.35      0.29       706\n",
      "weighted avg       0.47      0.50      0.38       706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## split dataset\n",
    "dtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3)\n",
    "\n",
    "## get target\n",
    "y_train = dtf_train[\"y\"].values\n",
    "y_test = dtf_test[\"y\"].values\n",
    "\n",
    "## Count (classic BoW)\n",
    "vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "corpus = dtf_train[\"text_clean\"]\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "y = dtf_train[\"y\"]\n",
    "\n",
    "X_names = vectorizer.get_feature_names()\n",
    "\n",
    "p_value_limit = 0.85\n",
    "\n",
    "dtf_features = pd.DataFrame()\n",
    "\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "    \n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    \n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    \n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "    \n",
    "X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "\n",
    "for cat in np.unique(y):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\"  . selected features:\",\n",
    "         len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "    print(\"  . top features:\", \",\".join(\n",
    "dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:5]))\n",
    "    print(\" \")\n",
    "\n",
    "#create a list to store the top features found\n",
    "features = \"\"\n",
    "\n",
    "features += \" \".join(dtf_features[dtf_features[\"y\"]=='Failure'][\"feature\"].values[:25])\n",
    "\n",
    "features += \" \".join(dtf_features[dtf_features[\"y\"]=='Average'][\"feature\"].values[:25])\n",
    "\n",
    "features += \" \".join(dtf_features[dtf_features[\"y\"]=='Success'][\"feature\"].values[:30])\n",
    "\n",
    "features = features.split()\n",
    "\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "\n",
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "## train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "## test\n",
    "X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "\n",
    "# classes = np.unique(y_test)\n",
    "# y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "# # create a column with only key words that are from the feature list created above\n",
    "movies_df['features'] = dtf['text_clean'].apply(lambda x: ''.join([word for word in x.split() if word in (features)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below code was taken from: \n",
    "# # https://towardsdatascience.com/dealing-with-list-values-in-pandas-dataframes-a177e534f173\n",
    "# #Create empty dict\n",
    "\n",
    "feature_dict = {}\n",
    "\n",
    "\n",
    "# Loop through all the tags\n",
    "for i, item in enumerate(features):\n",
    "    # Apply boolean mask\n",
    "    feature_dict[item] = movies_df['features'].apply(lambda x: item in x)\n",
    "\n",
    "# Return the results as a dataframe, change True and false values to 0 and 1\n",
    "feature_frame =  pd.DataFrame(feature_dict)\n",
    "feature_frame = feature_frame.astype(int)\n",
    "\n",
    "# #merge the genre dataframe back with the original one\n",
    "movies_df = pd.merge(movies_df, feature_frame, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More recent movies are more likely to have a predictive value for what features in future movies are likely to lead to success.  1999 was chosen as the initial cut off date because it was the low end of the interquartile range from the orignal set of data.  However, both a wider and narrower date range will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model1.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe to a csv file so that it can be used in analysis in another notebook\n",
    "movies_df.fillna({'actor_3_facebook_likes': movies_df.actor_3_facebook_likes.mean(),\n",
    "                                 'actor_2_facebook_likes': movies_df.actor_2_facebook_likes.mean(),\n",
    "                                 'facenumber_in_poster': movies_df.facenumber_in_poster.mean(), \n",
    "                                 'actor_1_facebook_likes': movies_df.actor_1_facebook_likes.mean(), \n",
    "                                 'aspect_ratio': movies_df.aspect_ratio.mean(), \n",
    "                                 'content_rating': movies_df.aspect_ratio.mode(), \n",
    "                                 'actor_2_name': 'unknown', 'actor_1_name': 'unknown',\n",
    "                                 'actor_3_name': 'unknown','plot_keywords': 'missing'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for movies that are after 1999\n",
    "movies_df = movies_df.loc[movies_df['title_year'] >= 1999]\n",
    "movies_df = movies_df.drop(columns = ['color', 'num_voted_users', 'country', 'num_user_for_reviews', 'num_critic_for_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store cleaned data in a csv to be used in analysis\n",
    "movies_df.to_csv(\"movies_df_cleaned.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates a profile document which analyses each of the variables in the dataframe \n",
    "# uncomment the lines below to run the profile report again\n",
    "\n",
    "# profitable_profile = ProfileReport(recent_profitable_movies, title = \"Movie Report\", explorative=True)\n",
    "# profitable_profile.to_file(\"profitable.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
